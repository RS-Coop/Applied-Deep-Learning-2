{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40bc6d0",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43aa414",
   "metadata": {},
   "source": [
    "In this notebook we will be investigating a number of reinforcement learning (RL) environments using the open-source RL library [OpenAI Gym](https://gym.openai.com/) (gym). I will not be training any models or agents as my main goal is to get a strong understanding of how to work in this setting, and get exposed to potential environments that would be interesting to explore further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b601ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import tf_agents as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3370d67",
   "metadata": {},
   "source": [
    "# OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32630320",
   "metadata": {},
   "source": [
    "We will begin by walking through \"Getting Started\" guide in the gym documentation. The first environment we consider is the classic Cart-Pole problem. In this problem the goal is to keep the pole balanced up-right on the cart.\n",
    "\n",
    "In the code below we instantiate the environment, render it, take a random action, and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d74942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rs-coop/anaconda3/envs/dl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2193ef",
   "metadata": {},
   "source": [
    "Okay, so that was fairly chaotic, but it gets the point across -- cool! We are also informed by the documentation that we should ignore the warning we recieve about calling step().\n",
    "\n",
    "Lets try one of the other environments they suggest..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ae092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbcb58",
   "metadata": {},
   "source": [
    "We didn't run that one for quite as long, but we can see the goal is to get the cart out of the valley and to the flag. Clearly this will require some amount of higher-level reasoning because you have to rock back-and-forth in order to accomplish the goal.\n",
    "\n",
    "Lets look at a few more cool environments before we hone in on some more specifics. I think the [Atari games](https://gym.openai.com/envs/#atari) are really interesting, so I wil go with that. It requires a few extra dependencies, but they are easy to install.\n",
    "\n",
    "For each Atari game there are two versions: one where the input is the machine RAM (128 bytes), and one where the input is the screen. At the moment this doesn't matter, but it is an important consideration when building a model. Pitfall is a classic, so I will take a look a that. Interestingly this wasn't one of the games listed in the plot from the \"Human-level Control through Deep Reinforcement Learning\" class slides, so I am not sure where things stand in terms of RL agent's capabilities with this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fb321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pitfall-ram-v0')\n",
    "env.reset()\n",
    "for _ in range(5000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ed14b",
   "metadata": {},
   "source": [
    "At least on my system, the rendered image is much too small to see. In looking for a solution to this I found the following two resources: this [Git issue](https://github.com/openai/gym/issues/550) and this [Git gist](https://gist.github.com/mttk/74dc6eaaea83b9f06c2cc99584d45f96). They are essentially the same fix, and I have implemented this in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e58829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.classic_control import rendering\n",
    "\n",
    "def upsample(rgb_array, k, l):\n",
    "    if k<=0 or l<=0:\n",
    "        print('Scale factors must be greater than 0!')\n",
    "        return rgb_array\n",
    "    \n",
    "    return np.repeat(np.repeat(rgb_array, k, axis=0), l, axis=1)\n",
    "\n",
    "def renderEnv(env, steps=1000, scale=4):\n",
    "    viewer = rendering.SimpleImageViewer()\n",
    "    env.reset()\n",
    "    for _ in range(steps):\n",
    "        env.step(env.action_space.sample())\n",
    "        \n",
    "        rgb = env.render('rgb_array')\n",
    "        upsampled = upsample(rgb, scale, scale)\n",
    "        \n",
    "        viewer.imshow(upsampled)\n",
    "        \n",
    "    viewer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21d71e",
   "metadata": {},
   "source": [
    "Essentially, we just upscale the images before rendering them. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967081a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pitfall-ram-v0')\n",
    "renderEnv(env, steps=1000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c4e5f",
   "metadata": {},
   "source": [
    "Okay, that looks much better, and the scale can be adjusted as necessary to make the image larger. One last environment that I found, which I thought looked interesting, was the Lunar Lander envrionment. It has both discrete and continuous control versions -- I will consider the continuous variant.\n",
    "\n",
    "Turns out that this also requires some extra dependencies. Specifically, I used conda to install swig and pip to install box2d-py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9dc1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b8d142",
   "metadata": {},
   "source": [
    "Becuase we are just naively taking a random action we don't actually end the episode when we should (i.e. when the lander crashes). This partially relates to that warning we got much earlier, and really it seems like a good time to get a bit more into the details. Up to now we have just been looking at environments with agents making random decisions, but that is not our long-term goal.\n",
    "\n",
    "We will take the Lunar Lander Continuous environment as our example and look a bit beyond simply rendering. According to the documentation the *step()* function returns a plethora of useful information that includes environment observations and rewards. The *reset()* function gets things started by returning and initial observation, so lets start there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2761cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00512733  1.4206668  -0.51935774  0.43317077  0.00594808  0.11764237\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "lander = gym.make('LunarLanderContinuous-v2')\n",
    "obst = lander.reset() #i.e. fruit\n",
    "\n",
    "print(obst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c42859",
   "metadata": {},
   "source": [
    "Currently this doesn't look like much, but we can consult the environment specific documentation to get more information. Doing so tells us that the first two values are the current (x,y) coordinates of the lander, the next two are the (x,y) velocities of the lander, then the angle, the angular speed, and finally boolean values for if the left and right legs have contacted the ground. It should be noted that some of this information had to be obtained directly from the source code.\n",
    "\n",
    "This is the information that the agent will have available to make an action decision. It is worth pointing out that, in Pitfall for example, if the observation is the game screen, then the only observation information we would have is a matrix representing pixel values.\n",
    "\n",
    "Now let't look at the information that *step()* returns, which will include an observation, but also other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc66e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.17200371560562644, Done: False\n"
     ]
    }
   ],
   "source": [
    "o, r, done, info = lander.step(lander.action_space.sample())\n",
    "\n",
    "print(f'Reward: {r}, Done: {done}')\n",
    "lander.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be1e2a",
   "metadata": {},
   "source": [
    "We have already examined an observation, and the info variable contains diagnostic infromation, so we will focus on the reward and done values shown above. The reward is the amount of reward generated by taking the previous action, which at this point is a random action. The documentation gives detailed information on how the reward is calculated. For example, crashing would be worth -100 and safely coming to rest would be worth 100. In this case there is always a reward being generated, but in other environments this may not be the case. The done value indicates whether the episode has ended, so in this case whether the lander has landed or crashed. We can see what happens when we use this information in the simulation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0eea160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "for t in range(1000):\n",
    "    env.render()\n",
    "    _, _, done, _ = env.step(env.action_space.sample())\n",
    "    if done:\n",
    "        print(\"Episode finished.\")\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbafb54",
   "metadata": {},
   "source": [
    "Up to now we have been taking actions by randomly sampling from the environment action space, but how is this space even defined and how can we make more informed decisions? \n",
    "\n",
    "According to the documentation every environment must have an associated action and observation space. In the case of actions, these are all the valid actions an agent can take and they will have some kind of effect on the environment. In the case of Lunar Lander an action is a combination of two real values in $[-1,1]$. First is the main engine throttle, where -1 is off and $[0,1]$ corresponds to 50% through 100% throttle -- apparently the engine only works above 50%. Second is the lateral engine throttle, with $[-1,-0.5]$ firing the left engine, $[0.5,1]$ firing the right engine, and the remainder indicating no engine should be fired.\n",
    "\n",
    "In the discrete formulation we loose these continuous values and instead are restricted to four states. Main engine fire at full throttle, left eninge fire, right engine fire, and no engine fire.\n",
    "\n",
    "Lets validate what we have discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6c32a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Action: [-0.89128834 -0.42673907]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "action = env.action_space.sample()\n",
    "print(f'Random Action: {action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17cb012",
   "metadata": {},
   "source": [
    "It looks like the random action we have sampled does not fire any of the engines, so the lander will just continue its current trajectory. Lets try specifying a more interesting action that will have some effect on the environment over a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46cbb438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: [ 0.00773897  1.4076171   0.7838566  -0.14682311 -0.00896072 -0.17755531\n",
      "  0.          0.        ]\n",
      "\n",
      "Action: [ 1.  -0.7]\n",
      "\n",
      "Final Observation: [0.16171427 1.4709758  0.6856318  0.31994307 0.1292875  0.40468216\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "o = env.reset()\n",
    "print(f'Initial observation: {o}\\n')\n",
    "action = np.array([1.0, -.7])\n",
    "print(f'Action: {action}\\n')\n",
    "\n",
    "for i in range(20):\n",
    "    o, _, _, _ = env.step(action)\n",
    "    \n",
    "print(f'Final Observation: {o}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb87e66",
   "metadata": {},
   "source": [
    "The action we have taken over 20 steps is to fire the main engine at 100% and fire the left engine. We expect to see the x and y coordinates (first and second values in the array) increase, and this is exactly what happens. We can also see that the horizontal and vertical speeds (the next two values in the array) increase as further verification.\n",
    "\n",
    "This pretty much sums up everything we need to know about gym, with more of the complicated tasks coming from modeling and training an agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

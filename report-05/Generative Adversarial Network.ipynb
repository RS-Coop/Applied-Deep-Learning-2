{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interstate-turkey",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-valentine",
   "metadata": {},
   "source": [
    "In this notebook we will be investigating Generative Adversarial Networks (GANs). We will birefly described their formulation, and then we will build a network to generate handwritten digits based on training from the MNIST dataset. Our model will not deviate too much from the classic setup for a GAN, although we will introduce a few more modern tweaks. I have used [this TensorFlow tutorial](https://www.tensorflow.org/tutorials/generative/dcgan) and [this Keras tutorial](https://keras.io/examples/generative/dcgan_overriding_train_step/) to help me in my work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-frank",
   "metadata": {},
   "source": [
    "# Mathematical Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-purpose",
   "metadata": {},
   "source": [
    "GANs are suprisingly straightforward, although they certainly can become much more complicated as one introduces more modifications or attempts to stabalize them in some way. The idea is that you train two networks simultaneously. One network is the generator which produces fake images, and the other netowrk is the discriminator which distinguishes between real and fake images.\n",
    "\n",
    "For the generator we assume that it generates an image based on some latent input $z$ with prior distribution $p_z$. This pretty much brings us directly to the loss function for our model where G is the generator and D is the discriminator.\n",
    "\n",
    "$$\n",
    "\\min_G\\max_D \\mathrm{E}_{x\\sim p_{data}(x)}[\\log D(x)] + \\mathrm{E}_{z\\sim p_{z}(z)}[\\log(1-D(G(z))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-manitoba",
   "metadata": {},
   "source": [
    "Looking at the inner maximization we can see that we want the discriminator to correctly identify the real images as real and the fake images as fake. The minimization for the generator is then about fooling the descriminator into thinking that its generated images are real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-perry",
   "metadata": {},
   "source": [
    "The only other major thing we need is the architecture of the generator and descriminator. This will be determined by the data, so let's move on to building a network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-blocking",
   "metadata": {},
   "source": [
    "# Building a Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prospective-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-handle",
   "metadata": {},
   "source": [
    "We went through the dataset a bit more extensively in the *Variational Auto-Encoder* notebook, so we will move through it quickly here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bound-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download\n",
    "(train, _), (test, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "imgs = np.concatenate((train, test), axis=0).astype('float32')\n",
    "# labels = np.concatenate((train[1], test[1]), axis=0)\n",
    "\n",
    "#Reshape and rescale\n",
    "imgs = imgs.reshape((imgs.shape[0], 28, 28, 1))/255\n",
    "\n",
    "#Turn into TF Dataset\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "size = imgs.shape[0]\n",
    "batch_size = 256\n",
    "\n",
    "dataset = (tf.data.Dataset.from_tensor_slices(imgs).shuffle(size)\n",
    "           .batch(batch_size).prefetch(AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quick-advertiser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Look at a picture\n",
    "fig, ax = plt.subplots(2,2,figsize=(8,8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax[i,j].imshow(train[np.random.randint(1,100)], cmap='gray')\n",
    "\n",
    "        ax[i,j].tick_params(axis='both', labelbottom=False, \n",
    "                       labelleft=False, bottom=False, left=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-garden",
   "metadata": {},
   "source": [
    "Now we will build our model. We set our latent dimension at 100 and our base number of filters is 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adjustable-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "base = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-local",
   "metadata": {},
   "source": [
    "The generator is a transposed CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cultural-produce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 6272)              627200    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 6272)              25088     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 128)         147456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         288       \n",
      "=================================================================\n",
      "Total params: 893,088\n",
      "Trainable params: 880,096\n",
      "Non-trainable params: 12,992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "    \n",
    "    keras.layers.Dense(units=7*7*4*base, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('swish'),\n",
    "    \n",
    "    keras.layers.Reshape((7,7,4*base)),\n",
    "    \n",
    "    keras.layers.Conv2DTranspose(filters=4*base, kernel_size=3, \n",
    "                                strides=1, padding='same', \n",
    "                                use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('swish'),\n",
    "    \n",
    "    keras.layers.Conv2DTranspose(filters=2*base, kernel_size=3, \n",
    "                                strides=2, padding='same', \n",
    "                                use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('swish'),\n",
    "    \n",
    "    keras.layers.Conv2DTranspose(filters=base, kernel_size=3, \n",
    "                                strides=2, padding='same', \n",
    "                                use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('swish'),\n",
    "    \n",
    "    keras.layers.Conv2DTranspose(filters=1, kernel_size=3, \n",
    "                                strides=1, padding='same', \n",
    "                                use_bias=False, activation='sigmoid')\n",
    "])\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-reminder",
   "metadata": {},
   "source": [
    "The discriminator is a standard CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faced-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6273      \n",
      "=================================================================\n",
      "Total params: 99,713\n",
      "Trainable params: 99,329\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28,28,1)),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=base, kernel_size=3, strides=1, \n",
    "                        padding='same'),\n",
    "    keras.layers.Activation('swish'),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=2*base, kernel_size=3, strides=2, \n",
    "                        padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('swish'),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=4*base, kernel_size=3, strides=2, \n",
    "                        padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('swish'),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-consciousness",
   "metadata": {},
   "source": [
    "The following subclasses a Keras model to allow us to implement the somewhat more complicated training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "driving-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.G = generator\n",
    "        self.D = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def compile(self, g_optimizer, d_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        \n",
    "        self.g_optim = g_optimizer\n",
    "        self.d_optim = d_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "        self.g_metric = keras.metrics.Mean(name='g_loss')\n",
    "        self.d_metric = keras.metrics.Mean(name='d_loss')\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_metric, self.d_metric]\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, real_imgs):\n",
    "        batch_size = tf.shape(real_imgs)[0]\n",
    "        \n",
    "        noise = tf.random.normal((batch_size, self.latent_dim))\n",
    "        \n",
    "        labels = tf.concat([tf.zeros((batch_size,1)), \n",
    "                            tf.ones((batch_size,1))], axis=0)\n",
    "        labels += 0.05*tf.random.uniform(tf.shape(labels))\n",
    "        \n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            gen_imgs = self.G(noise)\n",
    "            \n",
    "            imgs = tf.concat([gen_imgs, real_imgs], axis=0)\n",
    "            \n",
    "            predictions = self.D(imgs)\n",
    "            \n",
    "            g_loss = self.loss_fn(labels[batch_size:], \n",
    "                                  predictions[:batch_size])\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "            \n",
    "        g_vars = self.G.trainable_variables\n",
    "        d_vars = self.D.trainable_variables\n",
    "\n",
    "        g_grad = g_tape.gradient(g_loss, g_vars)\n",
    "        d_grad = d_tape.gradient(d_loss, d_vars)\n",
    "\n",
    "        self.g_optim.apply_gradients(zip(g_grad, g_vars))\n",
    "        self.d_optim.apply_gradients(zip(d_grad, d_vars))\n",
    "        \n",
    "        #Update metrics\n",
    "        self.g_metric.update_state(g_loss)\n",
    "        self.d_metric.update_state(d_loss)\n",
    "        \n",
    "        return {'g_loss': self.g_metric.result(), \n",
    "                'd_loss': self.d_metric.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "broken-brush",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "274/274 [==============================] - 40s 137ms/step - g_loss: 3.3623 - d_loss: -0.5269\n",
      "Epoch 2/30\n",
      "274/274 [==============================] - 39s 141ms/step - g_loss: 269.0355 - d_loss: -42.2445\n",
      "Epoch 3/30\n",
      "274/274 [==============================] - 40s 146ms/step - g_loss: 1787.2091 - d_loss: -42.9227\n",
      "Epoch 4/30\n",
      "274/274 [==============================] - 41s 148ms/step - g_loss: 1238.6931 - d_loss: -32.1834\n",
      "Epoch 5/30\n",
      "274/274 [==============================] - 41s 150ms/step - g_loss: 1543.6191 - d_loss: -9.3041\n",
      "Epoch 6/30\n",
      "274/274 [==============================] - 41s 151ms/step - g_loss: 751.7455 - d_loss: 9.6160\n",
      "Epoch 7/30\n",
      "274/274 [==============================] - 42s 154ms/step - g_loss: 431.4754 - d_loss: 0.6728\n",
      "Epoch 8/30\n",
      "274/274 [==============================] - 42s 154ms/step - g_loss: 315.0453 - d_loss: -3.5333\n",
      "Epoch 9/30\n",
      "274/274 [==============================] - 42s 154ms/step - g_loss: 253.2318 - d_loss: -4.8933\n",
      "Epoch 10/30\n",
      "274/274 [==============================] - 43s 156ms/step - g_loss: 216.6697 - d_loss: -4.0383\n",
      "Epoch 11/30\n",
      "274/274 [==============================] - 43s 158ms/step - g_loss: 190.6653 - d_loss: -3.0879\n",
      "Epoch 12/30\n",
      "274/274 [==============================] - 43s 158ms/step - g_loss: 171.9255 - d_loss: -2.7706\n",
      "Epoch 13/30\n",
      "274/274 [==============================] - 43s 157ms/step - g_loss: 151.6782 - d_loss: -3.2387\n",
      "Epoch 14/30\n",
      "274/274 [==============================] - 43s 157ms/step - g_loss: 139.0888 - d_loss: -3.7321\n",
      "Epoch 15/30\n",
      "274/274 [==============================] - 43s 157ms/step - g_loss: 137.1945 - d_loss: -3.6457\n",
      "Epoch 16/30\n",
      "274/274 [==============================] - 43s 158ms/step - g_loss: 146.0817 - d_loss: -2.1749\n",
      "Epoch 17/30\n",
      "274/274 [==============================] - 44s 160ms/step - g_loss: 144.2135 - d_loss: -1.0813\n",
      "Epoch 18/30\n",
      "274/274 [==============================] - 44s 159ms/step - g_loss: 133.9966 - d_loss: -1.1028\n",
      "Epoch 19/30\n",
      "274/274 [==============================] - 44s 158ms/step - g_loss: 126.1948 - d_loss: -1.3060\n",
      "Epoch 20/30\n",
      "274/274 [==============================] - 44s 159ms/step - g_loss: 116.4675 - d_loss: -1.0229\n",
      "Epoch 21/30\n",
      "274/274 [==============================] - 44s 159ms/step - g_loss: 109.0267 - d_loss: 0.0187\n",
      "Epoch 22/30\n",
      "274/274 [==============================] - 44s 160ms/step - g_loss: 100.7029 - d_loss: 1.0539\n",
      "Epoch 23/30\n",
      "274/274 [==============================] - 44s 160ms/step - g_loss: 90.6242 - d_loss: 1.6041\n",
      "Epoch 24/30\n",
      "274/274 [==============================] - 44s 161ms/step - g_loss: 80.5802 - d_loss: 1.7051\n",
      "Epoch 25/30\n",
      "274/274 [==============================] - 44s 161ms/step - g_loss: 73.2533 - d_loss: 1.3399\n",
      "Epoch 26/30\n",
      "274/274 [==============================] - 44s 161ms/step - g_loss: 70.0484 - d_loss: 0.9535\n",
      "Epoch 27/30\n",
      "274/274 [==============================] - 44s 161ms/step - g_loss: 67.2225 - d_loss: 0.7062\n",
      "Epoch 28/30\n",
      "274/274 [==============================] - 45s 162ms/step - g_loss: 66.6656 - d_loss: 0.6816\n",
      "Epoch 29/30\n",
      "274/274 [==============================] - 44s 162ms/step - g_loss: 67.5663 - d_loss: 0.6813\n",
      "Epoch 30/30\n",
      "274/274 [==============================] - 44s 160ms/step - g_loss: 66.3024 - d_loss: 0.9248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feb70682640>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan = GAN(generator, discriminator, latent_dim)\n",
    "gan.compile(tf.keras.optimizers.Adam(1e-5), \n",
    "            tf.keras.optimizers.Adam(1e-4),\n",
    "            keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "\n",
    "gan.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-acrobat",
   "metadata": {},
   "source": [
    "I was originally planning to train for 50 epochs, but that was taking too long, so I settled with 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "valid-brake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAccAAAHBCAYAAAAcpXCvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVnElEQVR4nO3dbWyddfkH8HPWdp1b2QPbdINlI6PbUINOQ5CFGOMUQYG4RGPAGCTT+IhIZEpIZowxRJTF+BAxYRIyxUQiiUSzTAUyo4kCZuBAYQoOhjK2ZSDseetO7/8LX/zj5a5f27P2tOf083m5r6fnWrv7fL3JffVXr6qqBgD8vynjPQAATDTKEQAC5QgAgXIEgEA5AkCgHAEg6C6F9Xrdngf8r/1VVc0f7yFGyvUM/6uqqvqp/tydI4zcrvEeABhbyhEAAuUIAIFyBIBAOQJAoBwBIFCOABAoRwAIlCMABMoRAALlCACBcgSAQDkCQKAcASBQjgAQKEcACJQjAATKEQAC5QgAgXIEgEA5AkCgHAEgUI4AEChHAAiUIwAEyhEAAuUIAEH3eA9A+1mwYEGa/fSnP02zvr6+4tfdsGFDU18XYLS5cwSAQDkCQKAcASBQjgAQKEcACJQjAAT1qqrysF7Pww4xbdq0NDt58mRTWSd4+umn06y/v7+Fk/zH8ePH06y0IjJGP6dtVVVdMBZfeCxNhuuZ0dXV1ZVmjUajhZOMnaqq6qf6c3eOABAoRwAIlCMABMoRAALlCACBcgSAoCNO5ZgyJe/4d7/73cXX3nHHHWl24MCBNLv55pvTbPPmzcX3nCje8IY3pNl4rGuU9Pb2ptmrr76aZvPnz0+zI0eOnNZM0C7mzp2bZjfccEOaffCDH0yzLVu2FN/zlltuSbOXXnqp+NqJwJ0jAATKEQAC5QgAgXIEgEA5AkCgHAEg6IhVjnXr1qXZV7/61eJre3p6mnrPW2+9Nc1+85vfpNnAwEBT79esev2Uv3C+VqvVao899lgLJxk7pZWbo0ePtnASGD/vfe970+zHP/5xms2aNSvNjh07lmavf/3ri/OcccYZaWaVAwDakHIEgEA5AkCgHAEgUI4AEChHAAiUIwAEbbPn+JrXvCbNPv/5z6fZUHuMpeOuSpYvX55mDz/8cJqtXbs2zbZv355mVVUNb7BgzZo1adbd3TY//uK+1apVq9Ks2e8bk8sjjzySZm9605vSrPT5UvpsaTQaaVb6N1vKmt3ZLn3N0nV37733Fr/uiy++2NQ8E4U7RwAIlCMABMoRAALlCACBcgSAQDkCQNA2z/IfP348zUqPG5eObBpK6bWlNYg3vvGNafbDH/4wzb785S+n2f33359mM2fOTLNPfvKTadbsGkuzSj/DWq1W27VrV5qVjscZHBxseiYmj/3796fZ3LlzWzhJrdbV1dXS9ysprXJs3Lgxze68886xGGfCcOcIAIFyBIBAOQJAoBwBIFCOABAoRwAI2maVo+TPf/5zms2bN6/42hkzZqRZadWh9PhzaQVkxYoVaVZ6NPoXv/hFmpUeC+/v70+z0gpE6e9Q+ru/9NJLaXbjjTemWa1Wq/3yl79MM+saDEfpmp01a1YLJ2kfhw8fTrPvfve7LZxkYnHnCACBcgSAQDkCQKAcASBQjgAQKEcACOpDrCTkYZsY6lSO0vrE29/+9jRbsGBBmvX29qZZ6ftdUjpRoLTmUHq/vr6+NCudoHH55Zen2eOPP55mJ0+eTLM2s62qqgvGe4iR6oTreSil6710es/UqVPHYpyWajQaafbrX/86zUrX82RQVdUp/9G4cwSAQDkCQKAcASBQjgAQKEcACJQjAAQdv8oxlNKj36VVhx/96EdpduGFF6bZnDlz0qx0osCJEyfS7ODBg2n21FNPpdnWrVvT7IEHHkizhx9+OM0mCascbeicc85Js507d6bZUOtgrXTo0KE0O+uss9Ks9Bkx2VnlAIBhUo4AEChHAAiUIwAEyhEAAuUIAMGkX+VoVulUjtKax6pVq9Ksp6cnzUqrHLt27Uqz73znO2m2adOmNCudoNHsySIdxCpHh/nWt76VZldddVWavfa1r02zgYGBNOvq6kqzz33uc2l2xx13pJnrsjlWOQBgmJQjAATKEQAC5QgAgXIEgEA5AkBglaNJpUexP/ShD6XZN7/5zTTr7e1Ns9Jj4Q899FCa3XTTTWn2j3/8I808Fl5klaPDlE7EWbhwYZpdeeWVafbss8+m2bJly9Ls9ttvT7PBwcE0ozlWOQBgmJQjAATKEQAC5QgAgXIEgEA5AkBglaNJpd/G/8gjj6TZ2WefnWal9ZDSI9yvvvpqmq1evTrNtm/fnmYUWeWgVqvVajNnzkyz0mdrX19fmr344ounNRMjY5UDAIZJOQJAoBwBIFCOABAoRwAIlCMABN3jPcBEVq+f8gnfWq1Wq/3pT39Ks0WLFqVZ6bf/l5TWPObMmZNm119/fZp97GMfa2oWYGgzZsxIs+nTp7dwEprhzhEAAuUIAIFyBIBAOQJAoBwBIFCOABBY5Sj41a9+lWaLFy9u4SRlpZWT973vfS2cBDpP6fp661vfmmZr1qxJs2nTpqXZpz71qWHNxdhy5wgAgXIEgEA5AkCgHAEgUI4AEChHAAgm/SpHd3f+LbjoootaOMnY6OvrS7PSSR+NRmMsxoEJqbSuUTr15sMf/nCarVq1Ks16e3vT7IwzzkizgwcPphmjy50jAATKEQAC5QgAgXIEgEA5AkCgHAEgsMpRWOUYGBho4SRjo/Tot3UN+I8zzzwzzdatW5dml19+eZq97nWvS7PBwcE0u+eee9LsiiuuaOprMnLuHAEgUI4AEChHAAiUIwAEyhEAAuUIAMGkX+U4fvx4ml122WVp9uCDD6bZ9OnT06y0OtKsqqrS7C9/+cuovx+0o9JpF7fddluaXXLJJWk2d+7cNJsyJb/3KGWXXnppmm3dujXNVq9enWbWtkbOnSMABMoRAALlCACBcgSAQDkCQKAcASCol9YA6vV6Hk5yy5cvT7Nrr702zT772c+m2bRp09KstAJy5MiRNCudGvC73/0uzSjaVlXVBeM9xEhN9ut55cqVaXbfffel2aJFi9KstJJRr9eHM9aIHDt2LM3OPffcNNu9e/eoz9Ipqqo65Q/KnSMABMoRAALlCACBcgSAQDkCQKAcASBQjgAQTPojq5r197//Pc2+9rWvpdnzzz+fZu985zvTbNWqVWm2Y8eONCvNCZ2mtHe4bNmyNOvq6kqzgYGBNOvt7R3eYKOkp6cnzb7xjW+k2dq1a9Os9PebzNw5AkCgHAEgUI4AEChHAAiUIwAEyhEAAkdWtVjpGJtSVjpup7TKcfTo0TQr/ewpcmRVGyqteZSOdrvsssvS7CMf+UiazZgxI81K13qj0UizvXv3pllp3at0ZNXg4GCaTQaOrAKAYVKOABAoRwAIlCMABMoRAALlCACBVY42UXr020pGy1nlmERKJ3asWLEizZYsWZJm1113XZpt3bo1zX7+85+n2c6dO9PMZ0TOKgcADJNyBIBAOQJAoBwBIFCOABAoRwAIrHLAyFnl4LRYzZo4rHIAwDApRwAIlCMABMoRAALlCACBcgSAoHu8BwCYbKxrTHzuHAEgUI4AEChHAAiUIwAEyhEAAuUIAIFyBIBAOQJAoBwBIFCOABAoRwAIlCMABMoRAIKhTuXYX6vVdrViEGgjS8Z7gCa5nuG/pddy3dEpAPDf/GdVAAiUIwAEyhEAAuUIAIFyBIBAOQJAoBwBIFCOABAoRwAIlCMABMoRAALlCACBcgSAQDkCQKAcASBQjgAQKEcACJQjAATKEQAC5QgAgXIEgEA5AkCgHAEgUI4AEChHAAiUIwAEyhEAAuUIAIFyBIBAOQJAoBwBIFCOABAoRwAIlCMABMoRAALlCACBcgSAoLsU1uv1qlWDQBvZX1XV/PEeYqRcz/C/qqqqn+rP3TnCyO0a7wGAsaUcASBQjgAQKEcACJQjAATKEQAC5QgAgXIEgEA5AkCgHAEgUI4AEChHAAiUIwAEyhEAAuUIAIFyBIBAOQJAoBwBIFCOABAoRwAIlCMABMoRAALlCACBcgSAQDkCQKAcASBQjgAQdI/3ABPZwoUL0+yLX/xims2ePTvN5s+fn2YnTpxIs+9973tp9tvf/jbNgNNTr9ebygYHB8diHFrEnSMABMoRAALlCACBcgSAQDkCQKAcASCoV1WVh/V6HnaIu+66K82uvfba1g0yhEajkWaLFy9Os927d4/FOJPdtqqqLhjvIUZqMlzPzXrggQfS7OKLL06zJ554Is3e9ra3pVnpc5fWqqrqlPs47hwBIFCOABAoRwAIlCMABMoRAALlCABBx69yzJkzp5jv3bs3zXp6ekZ7nDHxzDPPpNl5552XZqX1EIqscrShlStXptmjjz6aZs2evLF+/fo0+/rXv55mtJZVDgAYJuUIAIFyBIBAOQJAoBwBIFCOABB0j/cAo6H0qPUXvvCF4mu7urpGe5yWO+uss9LsPe95T5pt2bJlLMaBCemhhx5Ks9JnSMmUKfn9RelUjtL7ObFjYnDnCACBcgSAQDkCQKAcASBQjgAQKEcACJQjAAQdsefY29ubZrt27Sq+9tChQ2k2ffr0NGt2T6nZfapmv+add96ZZg8++GCaff/730+z0r4YjKfFixenWXd3az/u3vKWt6RZ6TPrxIkTadbX15dmP/vZz9Js9erVTb3f2WefnWavvPJKmnUCd44AEChHAAiUIwAEyhEAAuUIAIFyBICgI1Y5jh07lmabNm0qvnbv3r1ptn379jQrHRO1c+fONLv00kvTrPSY9tq1a9Osv78/zebMmZNmV199dZq9//3vT7Ply5en2Z49e9IMxtrUqVPTbCyOpyutWO3YsSPNTp48mWY9PT1p9pWvfCXNSsfTlZRWXF544YU0mzFjRlPv1y7cOQJAoBwBIFCOABAoRwAIlCMABMoRAIKOWOUoKT0yXavVaps3b06zwcHBNHv++eebmufuu+9Os9Jj6M8991ya3XTTTWl24YUXplnpNI/SWslTTz2VZkuXLk2zf//732kGo2HmzJlpNjAwkGal9YmSRqORZgcPHkyzadOmpdmZZ56ZZmvWrBnWXKOldDJRp3PnCACBcgSAQDkCQKAcASBQjgAQKEcACDp+laP0W/OHk4+20vsdP348zQ4cOJBm559/fpqVHhkvrXKUzJ49O81uu+22NPv4xz/e1PvBcJVOvSlde6WVjL/+9a9ptm7dujS7//7706zk0KFDafbkk0+mWWmNqlmt/nycSNw5AkCgHAEgUI4AEChHAAiUIwAEyhEAgnrpUd16vT55n+MdB6XViltvvTXNrrvuujRr9W/Vf+WVV9Jszpw5rRtkbG2rquqC8R5ipCbD9fzHP/4xzZYtW5Zm+/btS7OLLroozUorVmOhv78/zXbs2JFmXV1dTb3f/v3702z+/PlNfc2JpqqqU37wunMEgEA5AkCgHAEgUI4AEChHAAiUIwAEHX8qRzuZMiX//yof/ehH06zV6xolBw8eHO8R6GCla6RWq9WWLFmSZqUTLa6//vo0a/W6RskzzzyTZv/85z/T7Jxzzmnq/c4777ymXtcJ3DkCQKAcASBQjgAQKEcACJQjAATKEQACqxwTyMyZM9NsxowZLZykeXv27BnvEehg8+bNK+ZHjhxJsw0bNqTZ448/3vRME8XFF1+cZv/617/SrNFopNnLL798WjO1M3eOABAoRwAIlCMABMoRAALlCACBcgSAwCrHBLJx48Y06+vra+EkzbvmmmvGewTaXHd3/rG0bdu24mtLqx5Lly5Ns6qqhh5sgnviiSfSrF6vp1np+11aITt06NDwBmtT7hwBIFCOABAoRwAIlCMABMoRAALlCABBvfQIc71eb//nmyeYnp6eNNu9e3eaDXUawURReiy89Nv/28y2qqouGO8hRqpdrudHH300zVauXNn01z18+HCanXvuuWm2b9++pt9ztPX396fZ008/PervV+qHKVM6496qqqpT7rl0xt8OAEaRcgSAQDkCQKAcASBQjgAQKEcACKxytFgn/Jb7wcHBNOvq6mrhJOPGKscYOnnyZJqN1b+v++67L80+8IEPpFnpWmhWaUXiyJEjadbb2zvqs5R85jOfKeY/+MEPWjTJ6bHKAQDDpBwBIFCOABAoRwAIlCMABMoRAAKrHC325je/Oc0ee+yxNKvXT/m08bg4evRomk2fPr2Fk4wbqxynqfTvubTKMVYnQZTWqL797W+n2U9+8pM0K10nJ06cSLMbbrghzb70pS+lWauVuqNWa59TO6xyAMAwKUcACJQjAATKEQAC5QgAgXIEgKB7vAeYbG688cY0m0jrGqXHtCfJugZjqPTvq3TyRF9f35i858DAQJrNmjUrzVasWJFmL7zwQpqVTue54oor0mwiKa2j1GrlE1QajcZojzPq3DkCQKAcASBQjgAQKEcACJQjAATKEQCCSb/KsWDBgqayadOmpdm73vWuprIhTkhJs5LS49abN29Os/Xr1zf1fnC6br/99jT7xCc+UXxtaUWidEpG6USce++9N82efPLJNDt48GCa9fT0pNmVV16ZZr///e/TbOHChWnW7OfH4OBgml199dXF17bDukaJO0cACJQjAATKEQAC5QgAgXIEgEA5AkBQH2J9IA/bxPLly4v5li1b0mzJkiVpVlqRaPa30U+dOjXNSqcGXHLJJWn2hz/8Ic1Kj2lTtK2qqgvGe4iRapfruXT9nH/++cXX3nzzzWl2yy23pNm+ffvSbM+ePcX3bKXSSsaUKfm9zqc//ek027RpU5odO3YszUqfSe2kqqpTflPdOQJAoBwBIFCOABAoRwAIlCMABMoRAALlCABBR+w5Llq0KM3uuuuu4mvf8Y53pFl3d3MnepX2Bw8fPpxmpX2qjRs3ptmGDRuGNxijxZ7jOCnt8tVqtdrs2bPT7MCBA2lWumbtA3c2e44AMEzKEQAC5QgAgXIEgEA5AkCgHAEgaG5XYYKZN29empXWPIZSWnMpPd793HPPpdn69evT7J577hnWXDBZDbVW8fLLL7doEjqdO0cACJQjAATKEQAC5QgAgXIEgEA5AkDQEasce/fuTbO77767+NprrrkmzUqrHEePHk2zq666Ks3+9re/FecBYPy5cwSAQDkCQKAcASBQjgAQKEcACJQjAAT10rpCvV7PwwmkXq+nWU9PT/G1S5cuTbNnn302zbq6utLsyJEjxfek7W2rquqC8R5ipNrleoZWqqrqlAXizhEAAuUIAIFyBIBAOQJAoBwBIFCOABB0xCpHSWnNY6h8cHBwtMehM1jlgA5hlQMAhkk5AkCgHAEgUI4AEChHAAiUIwAE3UP9D7LTJxqNxqgP06wpU/KOt8oBwEi5cwSAQDkCQKAcASBQjgAQKEcACJQjAARDrXLsbzQau1oyyWmwckGLLRnvAZq0v1arTfjrGVoovZaLR1YBwGTkP6sCQKAcASBQjgAQKEcACJQjAAT/Bzf465DnNDxGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Look at a picture\n",
    "fig, ax = plt.subplots(2,2,figsize=(8,8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        noise = tf.random.normal((1, latent_dim))\n",
    "        gen_x = gan.G.predict(noise).reshape((28,28,1))\n",
    "        \n",
    "        ax[i,j].imshow(gen_x, cmap='gray')\n",
    "\n",
    "        ax[i,j].tick_params(axis='both', labelbottom=False, \n",
    "                       labelleft=False, bottom=False, left=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-liberty",
   "metadata": {},
   "source": [
    "Hmmm, things aren't looking too good. I may need to train for longer, or perhaps my model setup isn't quite tuned correctly. Clearly the generator is learning something, but I will need to investigate more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-portland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

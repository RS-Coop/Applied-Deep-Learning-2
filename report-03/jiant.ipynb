{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informal-chile",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-nowhere",
   "metadata": {},
   "source": [
    "In this notebook we will be investigating the [jiant](https://jiant.info/) NLP toolkit from the MLL group at NYU. We will conduct our exploration by using the toolkit to download, train, and evaluate two BERT based transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-bryan",
   "metadata": {},
   "source": [
    "NOTE: I eventually ran into multiple sets of problems with jiant while writing this notebook. Instead of just getting rid of the work I had already done I will just point out where things change and/or fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "undefined-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from jiant.proj.main.export_model import export_model\n",
    "import jiant.scripts.download_data.runscript as downloader\n",
    "import jiant.proj.simple.runscript as simple_run\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-helena",
   "metadata": {},
   "source": [
    "## Working with jiant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-evolution",
   "metadata": {},
   "source": [
    "jiant focuses on supporting model training and benchmarking for research and development. It allows integration with many state-of-the-art (or past SOTA) models, and it provides access to many common natural language datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-median",
   "metadata": {},
   "source": [
    "jiant works by having the user download a pre-trained model, and data associated with some target task. By interfacing with [Hugging Face](https://huggingface.co/) -- which hosts transformer models and NLP datasets -- jiant has access to hundreds of the most relevant and important such models and tasks. jiant then facilitates the user to fine-tune the chosen model on the new task and evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-associate",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-beach",
   "metadata": {},
   "source": [
    "We will examine two BERT based models: ALBERT and DistilBERT. In both cases we will use the base version. Each of these models was created in an attempt to reduce the size and increase the speed of the original BERT transformer.\n",
    "\n",
    "ALBERT has approached this task by factorizing the word embeddings, improving training by switching to sentence-order prediction instead of next-sentence predicition, and most importantly by sharing parameters across layers. DisilBERT, on the other hand, is simply a distilled version of the original BERT base model. This distillation process is well understood and works by training a student model to reproduce the results of a teacher model. In this case the teacher model is BERT and the student model is a reduced size version.\n",
    "\n",
    "The models have the following important attributes:\n",
    "- ALBERT\n",
    "    - Depth -> 12\n",
    "    - Embedding Dim -> 128\n",
    "    - Parameters -> 11M\n",
    "- DistilBERT\n",
    "    - Depth -> 6\n",
    "    - Embedding Dim -> 768\n",
    "    - Parameters -> 66M\n",
    "\n",
    "I think the most important thing to note form the numbers above is that ALBERT is deeper but has fewer pramaters and DistilBERT is shallower with more parameters. We are looking at this trade off between computational complexity and memory footprint. I expect DistilBERT to run faster and ALBERT  to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-locator",
   "metadata": {},
   "source": [
    "In the code below we download our two models to the \"core/models\" directory. In running it you may see a warning output, but this is just to make the user aware that the ALBERT model is to be used for a downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "attended-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download models to \"models\" dir\n",
    "m1 = 'albert-base-v2'\n",
    "m2 = 'distilbert-base-uncased'\n",
    "\n",
    "export_model(m1, './core/models/albert')\n",
    "export_model(m2, './core/models/distilbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-cable",
   "metadata": {},
   "source": [
    "jiant attempts to be very modular by considering a model as a combination of encoder and task-specific head. The encoder is a pre-trained language model such as the those we have imported above. The task-specific head is dependent on the task and dataset being considered -- with multiple heads possible for multi-task learning. We consider our down-stream task next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-stockholm",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-patrick",
   "metadata": {},
   "source": [
    "jiant considers a task to be the culmination of three things: raw data i/o, raw data transformations, and an evaluation scheme. \n",
    "\n",
    "The downstream task we will consider is the Stanford Question Answering Dataset version 2 ([SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/)). This dataset has 100,000 examples which consist of a question about a Wikipedia excerpt and an answer to that question from within the excerpt. As well, 50,000 adversarial examples are included which should have no answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-foster",
   "metadata": {},
   "source": [
    "Or so I thought. It turns out that the SQuAD Dataset only has a train split of data available for use. While this would be fine if we were more in control of the data ourselves (i.e. could craft our own splits), it causes problems down the road. Specifically, jiant is expecting there to be train, validation, and test splits, so without them things break.\n",
    "\n",
    "Thus the downstream task we will consider is the Large-scale ReAding Comprehension Dataset From Examination (RACE; people will do anything for an acronym). This dataset is similar to SQuAD in that it is a question and answer based task. It was compiled from English examinations in China and contains roughly 28,000 excerpts with 100,000 questions. This seems like a perfect dataset for training NLP models as the source it was compiled from was created to test NLP models, albeit in the form of highly advanced biological beings (i.e. humans)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-consultation",
   "metadata": {},
   "source": [
    "In the code below we download this dataset to the \"core/tasks\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "massive-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download SQuAD dataset\n",
    "task = 'race'\n",
    "\n",
    "downloader.download_data([task], './core/tasks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-pantyhose",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-concentration",
   "metadata": {},
   "source": [
    "We have our models and our target task, so the next step is training. We do exactly this using jiants \"simple\" api to define a configuration and run the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "substantial-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = simple_run.RunConfiguration(\n",
    "    run_name=f'{m1}_{task}',\n",
    "    exp_dir='core',\n",
    "    data_dir='core/tasks',\n",
    "    hf_pretrained_model_name_or_path=m1,\n",
    "    tasks=task,\n",
    "    train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    train_examples_cap=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greenhouse-grill",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from start\n",
      "  jiant_task_container_config_path: core/run_configs/albert-base-v2_race_config.json\n",
      "  output_dir: core/runs/albert-base-v2_race\n",
      "  hf_pretrained_model_name_or_path: albert-base-v2\n",
      "  model_path: core/models/albert/model/model.p\n",
      "  model_config_path: core/models/albert/model/config.json\n",
      "  model_load_mode: from_transformers\n",
      "  do_train: True\n",
      "  do_val: True\n",
      "  do_save: False\n",
      "  do_save_last: False\n",
      "  do_save_best: False\n",
      "  write_val_preds: False\n",
      "  write_test_preds: False\n",
      "  eval_every_steps: 0\n",
      "  save_every_steps: 0\n",
      "  save_checkpoint_every_steps: 0\n",
      "  no_improvements_for_n_evals: 0\n",
      "  keep_checkpoint_when_done: False\n",
      "  force_overwrite: False\n",
      "  seed: -1\n",
      "  learning_rate: 1e-05\n",
      "  adam_epsilon: 1e-08\n",
      "  max_grad_norm: 1.0\n",
      "  optimizer_type: adam\n",
      "  no_cuda: False\n",
      "  fp16: False\n",
      "  fp16_opt_level: O1\n",
      "  local_rank: -1\n",
      "  server_ip: \n",
      "  server_port: \n",
      "device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Using seed: 478504965\n",
      "{\n",
      "  \"jiant_task_container_config_path\": \"core/run_configs/albert-base-v2_race_config.json\",\n",
      "  \"output_dir\": \"core/runs/albert-base-v2_race\",\n",
      "  \"hf_pretrained_model_name_or_path\": \"albert-base-v2\",\n",
      "  \"model_path\": \"core/models/albert/model/model.p\",\n",
      "  \"model_config_path\": \"core/models/albert/model/config.json\",\n",
      "  \"model_load_mode\": \"from_transformers\",\n",
      "  \"do_train\": true,\n",
      "  \"do_val\": true,\n",
      "  \"do_save\": false,\n",
      "  \"do_save_last\": false,\n",
      "  \"do_save_best\": false,\n",
      "  \"write_val_preds\": false,\n",
      "  \"write_test_preds\": false,\n",
      "  \"eval_every_steps\": 0,\n",
      "  \"save_every_steps\": 0,\n",
      "  \"save_checkpoint_every_steps\": 0,\n",
      "  \"no_improvements_for_n_evals\": 0,\n",
      "  \"keep_checkpoint_when_done\": false,\n",
      "  \"force_overwrite\": false,\n",
      "  \"seed\": 478504965,\n",
      "  \"learning_rate\": 1e-05,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"optimizer_type\": \"adam\",\n",
      "  \"no_cuda\": false,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"local_rank\": -1,\n",
      "  \"server_ip\": \"\",\n",
      "  \"server_port\": \"\"\n",
      "}\n",
      "1\n",
      "Creating Tasks:\n",
      "    race (RaceTask): core/tasks/configs/race_config.json\n",
      "No optimizer decay for:\n",
      "  encoder.embeddings.LayerNorm.weight\n",
      "  encoder.embeddings.LayerNorm.bias\n",
      "  encoder.encoder.embedding_hidden_mapping_in.bias\n",
      "  encoder.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias\n",
      "  encoder.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias\n",
      "  encoder.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias\n",
      "  encoder.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias\n",
      "  encoder.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias\n",
      "  encoder.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight\n",
      "  encoder.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias\n",
      "  encoder.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias\n",
      "  encoder.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias\n",
      "  encoder.pooler.bias\n",
      "  taskmodels_dict.race.choice_scoring_head.dense.bias\n",
      "  taskmodels_dict.race.choice_scoring_head.out_proj.bias\n",
      "Using AdamW\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff260d500c34f21948690ecd29174cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=500.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9399dc6689144335950a36bd87c7e164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval (race, Val)', max=125.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36777dcd9564c429bf4eacf3b7cb226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval (race, Val)', max=125.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Best\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d0e9b2f45b476fa92708310fb0663e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Eval (race, Val)', max=1222.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"aggregated\": 0.4065889093513403,\n",
      "  \"race\": {\n",
      "    \"loss\": 1.3215613560522441,\n",
      "    \"metrics\": {\n",
      "      \"major\": 0.4065889093513403,\n",
      "      \"minor\": {\n",
      "        \"acc\": 0.4065889093513403\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "simple_run.run_simple(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-blocking",
   "metadata": {},
   "source": [
    "In order to make the training time feasible on my machine I set the batch size to 2, and capped the maximum training examples at 1,000. This is obviously very limiting to the actual capabilities of the network, but it is what I can do. In the above output I will point out the \"aggregated\" result at the end, which is specifying the final accuracy. We see that our model managed just above 40% compared to the 64% reported by the ALBERT paper. This isn't too bad given the limited training, and it is certainly only possible due to the pre-trained language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-migration",
   "metadata": {},
   "source": [
    "Next, we encounter our second problem. jiant had seemed to be a capable toolkit, but the more I worked with it the more issues I encountered. The sum of this is seen in trying to train our second model: DistilBERT. Even though jiant claims to support any transformer model that Hugging Face supports, it does not support this one. It is possible that the issue is easily fixed on the software side, but I am not so interested in doing that. When the model is being loaded in the second code cell below we encounter a key error that cannot be obviously fixed on the user side. I even tried to edit the jiant source code locally, but I couldn't find a clear fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "short-conservative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = simple_run.RunConfiguration(\n",
    "    run_name=f'{m2}_{task}',\n",
    "    exp_dir='core',\n",
    "    data_dir='core/tasks',\n",
    "    hf_pretrained_model_name_or_path=m2,\n",
    "    tasks=task,\n",
    "    train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    train_examples_cap=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "recent-wright",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from start\n",
      "  jiant_task_container_config_path: core/run_configs/distilbert-base-uncased_race_config.json\n",
      "  output_dir: core/runs/distilbert-base-uncased_race\n",
      "  hf_pretrained_model_name_or_path: distilbert-base-uncased\n",
      "  model_path: core/models/distilbert/model/model.p\n",
      "  model_config_path: core/models/distilbert/model/config.json\n",
      "  model_load_mode: from_transformers\n",
      "  do_train: True\n",
      "  do_val: True\n",
      "  do_save: False\n",
      "  do_save_last: False\n",
      "  do_save_best: False\n",
      "  write_val_preds: False\n",
      "  write_test_preds: False\n",
      "  eval_every_steps: 0\n",
      "  save_every_steps: 0\n",
      "  save_checkpoint_every_steps: 0\n",
      "  no_improvements_for_n_evals: 0\n",
      "  keep_checkpoint_when_done: False\n",
      "  force_overwrite: False\n",
      "  seed: -1\n",
      "  learning_rate: 1e-05\n",
      "  adam_epsilon: 1e-08\n",
      "  max_grad_norm: 1.0\n",
      "  optimizer_type: adam\n",
      "  no_cuda: False\n",
      "  fp16: False\n",
      "  fp16_opt_level: O1\n",
      "  local_rank: -1\n",
      "  server_ip: \n",
      "  server_port: \n",
      "device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Using seed: 1754488804\n",
      "{\n",
      "  \"jiant_task_container_config_path\": \"core/run_configs/distilbert-base-uncased_race_config.json\",\n",
      "  \"output_dir\": \"core/runs/distilbert-base-uncased_race\",\n",
      "  \"hf_pretrained_model_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"model_path\": \"core/models/distilbert/model/model.p\",\n",
      "  \"model_config_path\": \"core/models/distilbert/model/config.json\",\n",
      "  \"model_load_mode\": \"from_transformers\",\n",
      "  \"do_train\": true,\n",
      "  \"do_val\": true,\n",
      "  \"do_save\": false,\n",
      "  \"do_save_last\": false,\n",
      "  \"do_save_best\": false,\n",
      "  \"write_val_preds\": false,\n",
      "  \"write_test_preds\": false,\n",
      "  \"eval_every_steps\": 0,\n",
      "  \"save_every_steps\": 0,\n",
      "  \"save_checkpoint_every_steps\": 0,\n",
      "  \"no_improvements_for_n_evals\": 0,\n",
      "  \"keep_checkpoint_when_done\": false,\n",
      "  \"force_overwrite\": false,\n",
      "  \"seed\": 1754488804,\n",
      "  \"learning_rate\": 1e-05,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"optimizer_type\": \"adam\",\n",
      "  \"no_cuda\": false,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"local_rank\": -1,\n",
      "  \"server_ip\": \"\",\n",
      "  \"server_port\": \"\"\n",
      "}\n",
      "1\n",
      "Creating Tasks:\n",
      "    race (RaceTask): core/tasks/configs/race_config.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.1.output.LayerNorm.weight\", \"encoder.layer.1.output.LayerNorm.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.2.output.LayerNorm.weight\", \"encoder.layer.2.output.LayerNorm.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.3.output.LayerNorm.weight\", \"encoder.layer.3.output.LayerNorm.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.4.output.LayerNorm.weight\", \"encoder.layer.4.output.LayerNorm.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.5.output.LayerNorm.weight\", \"encoder.layer.5.output.LayerNorm.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.6.output.LayerNorm.weight\", \"encoder.layer.6.output.LayerNorm.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.7.output.LayerNorm.weight\", \"encoder.layer.7.output.LayerNorm.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.8.output.LayerNorm.weight\", \"encoder.layer.8.output.LayerNorm.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.9.output.LayerNorm.weight\", \"encoder.layer.9.output.LayerNorm.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.10.output.LayerNorm.weight\", \"encoder.layer.10.output.LayerNorm.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.11.output.LayerNorm.weight\", \"encoder.layer.11.output.LayerNorm.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ff52fc1f49fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/school/cu-boulder/Applied-Deep-Learning-2/jiant/jiant/proj/simple/runscript.py\u001b[0m in \u001b[0;36mrun_simple\u001b[0;34m(args, with_continue)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mrunscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0mpy_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"simple_run_config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cu-boulder/Applied-Deep-Learning-2/jiant/jiant/proj/main/runscript.py\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(args, checkpoint)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mjiant_task_container_config_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjiant_task_container_config_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         )\n\u001b[0;32m--> 140\u001b[0;31m         runner = setup_runner(\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mjiant_task_container\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjiant_task_container\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cu-boulder/Applied-Deep-Learning-2/jiant/jiant/proj/main/runscript.py\u001b[0m in \u001b[0;36msetup_runner\u001b[0;34m(args, jiant_task_container, quick_init_out, verbose)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtaskmodels_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjiant_task_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtaskmodels_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[0;32m---> 92\u001b[0;31m         jiant_model_setup.delegate_load_from_path(\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mjiant_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_load_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         )\n",
      "\u001b[0;32m~/Documents/school/cu-boulder/Applied-Deep-Learning-2/jiant/jiant/proj/main/modeling/model_setup.py\u001b[0m in \u001b[0;36mdelegate_load_from_path\u001b[0;34m(jiant_model, weights_path, load_mode)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[1;32m     95\u001b[0m     \u001b[0mweights_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdelegate_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cu-boulder/Applied-Deep-Learning-2/jiant/jiant/proj/main/modeling/model_setup.py\u001b[0m in \u001b[0;36mdelegate_load\u001b[0;34m(jiant_model, weights_dict, load_mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \"\"\"\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mload_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"from_transformers\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         return load_encoder_from_transformers_weights(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         )\n",
      "\u001b[0;32m~/Documents/school/cu-boulder/Applied-Deep-Learning-2/jiant/jiant/proj/main/modeling/model_setup.py\u001b[0m in \u001b[0;36mload_encoder_from_transformers_weights\u001b[0;34m(encoder, weights_dict, return_remainder)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mremainder_weights_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_weights_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_remainder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mremainder_weights_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m    847\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.1.output.LayerNorm.weight\", \"encoder.layer.1.output.LayerNorm.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.2.output.LayerNorm.weight\", \"encoder.layer.2.output.LayerNorm.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.3.output.LayerNorm.weight\", \"encoder.layer.3.output.LayerNorm.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.4.output.LayerNorm.weight\", \"encoder.layer.4.output.LayerNorm.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.5.output.LayerNorm.weight\", \"encoder.layer.5.output.LayerNorm.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.6.output.LayerNorm.weight\", \"encoder.layer.6.output.LayerNorm.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.7.output.LayerNorm.weight\", \"encoder.layer.7.output.LayerNorm.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.8.output.LayerNorm.weight\", \"encoder.layer.8.output.LayerNorm.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.9.output.LayerNorm.weight\", \"encoder.layer.9.output.LayerNorm.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.10.output.LayerNorm.weight\", \"encoder.layer.10.output.LayerNorm.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.11.output.LayerNorm.weight\", \"encoder.layer.11.output.LayerNorm.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". "
     ]
    }
   ],
   "source": [
    "simple_run.run_simple(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-magazine",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-transformation",
   "metadata": {},
   "source": [
    "The goal of this investigation was to explore an established package for NLP research that speeds up some of the more mundane processes associated with deep learning. As well, I was interested in comparing the qualities of two similar BERT variations relevant to class. In the end this was mostly a waste of time. I certainly learned some things, but mostly I learned to be smarter when working with third-party packages like jiant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "biological-arrow",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-breed",
   "metadata": {},
   "source": [
    "We will be exploring the [spaCy](https://spacy.io/) toolkit for Natural Language Processing (NLP). My goal is to investigate a BPE (e.g. Wordpiece) like algorithm for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "diagnostic-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-relief",
   "metadata": {},
   "source": [
    "## Working with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-street",
   "metadata": {},
   "source": [
    "I will not dwell too much on describing what this package is, but I will highlight what seem to its most important qualities. spaCy is fast, well established, and focused. Being fast is quite straightforward; however, by well established I mean the package is used throughout the NLP community and it is well documented. By focused I mean the package is focused on quickly providing good results, and not necessarily on giving the user every single possible option.\n",
    "\n",
    "I will note that spaCy also has support for pre-trained transformer models through [Hugging Face](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-function",
   "metadata": {},
   "source": [
    "Let's look at some basic stuff with spaCy just to get an idea of how it works. We have previously downloaded the \"en_core_web_sm\" pipeline as specified in their documentation. This is a small english model trained on web data. We load it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cooked-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "minor-funds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People NOUN\n",
      "in ADP\n",
      "the DET\n",
      "U.S.A PROPN\n",
      "are AUX\n",
      "crazy ADJ\n",
      ", PUNCT\n",
      "do AUX\n",
      "n't PART\n",
      "you PRON\n",
      "think VERB\n",
      "! PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"People in the U.S.A are crazy, don't you think!\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-recorder",
   "metadata": {},
   "source": [
    "We can see that the pipeline easily parses a made up document. We have printed the segemented tokens and their part of speech. As an example we can see that \"U.S.A\" is a single token (which makes sense), and it is classified as a proper noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "blond-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S.A GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-delta",
   "metadata": {},
   "source": [
    "In the output above we can see that the pipeline can extract entities from documents. It has extracted the \"U.S.A\" entity from our toy example above and correctly labeled it as a geopolitical entity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-transformation",
   "metadata": {},
   "source": [
    "### BPE Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-springer",
   "metadata": {},
   "source": [
    "I am interested in looking at Byte Pair Encoding (BPE) for document tokenization. To do this I first used the following code to download the wikitext-103 dataset compiled from Wikipedia.\n",
    "\n",
    "```\n",
    "wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "unzip wikitext-103-raw-v1.zip\n",
    "```\n",
    "\n",
    "Then I used the following documentation from Hugging Face [here](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) and spaCy [here](https://spacy.io/usage/linguistic-features) to build a tokenizer which can implement BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sharing-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tk_bpe = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "tk_bpe.pre_tokenizer = Whitespace()\n",
    "\n",
    "files = [f'wikitext-103-raw/wiki.{split}.raw' for split in ['test', 'train', 'valid']]\n",
    "\n",
    "tk_bpe.train(trainer, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-opportunity",
   "metadata": {},
   "source": [
    "Let's also build a WordPiece tokenizer to compare the top-down vs bottom approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "steady-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "tk_wp = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "tk_wp.pre_tokenizer = Whitespace()\n",
    "\n",
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "\n",
    "tk_wp.train(trainer, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-medicine",
   "metadata": {},
   "source": [
    "We have built and trained our tokenizer on the downloaded dataset, so now lets try it out using spaCy. Below we define a class that will use the tokenizers we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cultural-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "\n",
    "class BPELike:\n",
    "    def __init__(self, vocab, tokenizer):\n",
    "        self.vocab = vocab\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = self._tokenizer.encode(text)\n",
    "        words = []\n",
    "        spaces = []\n",
    "        for i, (text, (start, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):\n",
    "            words.append(text)\n",
    "            if i < len(tokens.tokens) - 1:\n",
    "                # If next start != current end we assume a space in between\n",
    "                next_start, next_end = tokens.offsets[i + 1]\n",
    "                spaces.append(next_start > end)\n",
    "            else:\n",
    "                spaces.append(True)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-shell",
   "metadata": {},
   "source": [
    "First, we try out the BPE tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "visible-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = BPELike(nlp.vocab, tk_bpe)\n",
    "\n",
    "doc = nlp(\"Let's try to make an uncommon sentence. Whomever you might\\\n",
    "            be, considering youself to be a person, you must\\\n",
    "            unequivocally agree this sentence is absurdly strange.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "infrared-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's try to make an uncommon sentence. Whomever you might be, considering youself to be a person, you must unequivocally agree this sentence is absurdly strange. \n",
      "['Let', \"'\", 's', 'try', 'to', 'make', 'an', 'uncommon', 'sentence', '.', 'Wh', 'ome', 'ver', 'you', 'might', 'be', ',', 'considering', 'you', 'self', 'to', 'be', 'a', 'person', ',', 'you', 'must', 'une', 'qu', 'iv', 'oc', 'ally', 'agree', 'this', 'sentence', 'is', 'absurd', 'ly', 'strange', '.']\n"
     ]
    }
   ],
   "source": [
    "print(doc.text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-casino",
   "metadata": {},
   "source": [
    "For the most part we just see the words being split on whitespace and tokenized whole. For more uncommon words we see just why algorithms like this are effective. For example, \"Whomever\" was split into \"Wh\" , \"ome\", and \"ver\". Similarily the word \"unequivocally\" was also split into subwords.\n",
    "\n",
    "Next, lets tokenize the same sentence using the WordPiece method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "alike-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = BPELike(nlp.vocab, tk_wp)\n",
    "\n",
    "doc = nlp(\"Let's try to make an uncommon sentence. Whomever you might\\\n",
    "            be, considering youself to be a person, you must\\\n",
    "            unequivocally agree this sentence is absurdly strange.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fourth-suspension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's try to make an uncommon sentence. Who##me##ver you might be, considering you##self to be a person, you must un##equ##ivo##ca##ll##y agree this sentence is absurd##ly strange. \n",
      "['Let', \"'\", 's', 'try', 'to', 'make', 'an', 'uncommon', 'sentence', '.', 'Who', '##me', '##ver', 'you', 'might', 'be', ',', 'considering', 'you', '##self', 'to', 'be', 'a', 'person', ',', 'you', 'must', 'un', '##equ', '##ivo', '##ca', '##ll', '##y', 'agree', 'this', 'sentence', 'is', 'absurd', '##ly', 'strange', '.']\n"
     ]
    }
   ],
   "source": [
    "print(doc.text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-function",
   "metadata": {},
   "source": [
    "Interestingly, the result is almost identical except for the word \"unequivocally\". In the BPE case this was split into 5 components, but in this case it was split into 6. As well, the components themselves differ between the two methods. It is not necessarily clear if one method is better than another, but I would tend to say that WordPiece looks better by inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-cinema",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-travel",
   "metadata": {},
   "source": [
    "The NLP toolkit spaCy is very capable and easy to use. Incorporating packages likes this into regular workflows in research or industry seems almost necessary. It is this sort of thing that saves so much time and allows one to really get to the meat of their work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parallel-ethics",
   "metadata": {},
   "source": [
    "# Hard Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-electricity",
   "metadata": {},
   "source": [
    "I am interested in working through some of the mathematical formulation behind Hard Attention. I believe it is a helpful exercise, and I think the connections to reinforcement learning are intriguing. I will try to focus on aspects that I think are important, or that were helpful for me to understand. I reference the paper [*Show, Attend, and Tell*](https://arxiv.org/pdf/1502.03044.pdf) as well as our class notes.\n",
    "\n",
    "We start with some basic setup that is fairly agnostic to the specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-highlight",
   "metadata": {},
   "source": [
    "$$ y = \\{y_1,y_2,\\ldots,y_C\\},\\:\\:y_i\\in\\mathbb{R}^K $$\n",
    "$$ a = \\{a_1,a_2,\\ldots,a_L\\},\\:\\:a_i\\in\\mathbb{R}^D $$\n",
    "\n",
    "The input is $a$ and the output is $y$. For example this could be an image and a caption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-legislation",
   "metadata": {},
   "source": [
    "We then have our attention variable which is a Multinoulli r.v. as it is a vector with elements either 1 or 0 (on or off).\n",
    "\n",
    "$$ s_t = \\{s_{t,1},s_{t,2},\\ldots,s_{t,L}\\} $$\n",
    "\n",
    "Thus we either pay attention to the ith input or not, and we can determine this probability using the regular methods involving an attention function and softmax activations. We also note that $t$ indexes into the output y.\n",
    "\n",
    "Importantly this is where soft attention differs from hard attention. In hard attention you take this \"probability\" (the output of the softmax) and treat it as a weighting for the input $a$. Thus we see that soft attention is stochastic (i.e. random) while hard attention is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-assurance",
   "metadata": {},
   "source": [
    "Our next step is to write a loss function. We know we are trying to model $p(y|a)$, so the start is easy...\n",
    "\n",
    "$$ \\log p(y|a) = \\log(\\sum_{s}p(s|a)p(y|s,a)) $$\n",
    "\n",
    "Where this follows from total probability. Note that $s$ is a collection of $s_t$ as we are considering the whole output $y$. We then write the following to get our loss...\n",
    "\n",
    "$$ \\log\\sum_{s}p(s|a)p(y|s,a) \\geq \\sum_{s}p(s|a)\\log p(y|s,a) = L_s$$\n",
    "\n",
    "Becuase $\\log$ is a concave function this inequality is a direct result of Jensen's inequality where we treat our weights as $p(s|a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-italy",
   "metadata": {},
   "source": [
    "Now we need to differentiate with respect to some arbitrary weights $W$ in our model.\n",
    "\n",
    "$$ \\frac{\\partial L_s}{\\partial W} = \\sum_{s}p(s|a)\\frac{\\partial\\log p(y|s,a)}{\\partial W} + \\log p(y|s,a)\\frac{\\partial p(s|a)}{\\partial W} $$\n",
    "\n",
    "Then we note the following:\n",
    "$$ \\frac{\\partial \\log f(x)}{\\partial x} = \\frac{f'(x)}{f(x)} $$\n",
    "\n",
    "$$ \\implies \\frac{\\partial L_s}{\\partial W} = \\sum_{s}p(s|a)\\Big( \\frac{\\partial\\log p(y|s,a)}{\\partial W} +  \\log p(y|s,a)\\frac{\\partial \\log p(s|a)}{\\partial W} \\Big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-highland",
   "metadata": {},
   "source": [
    "At this point a good question to ask might be why is this useful? The answer is that we can apply Monte Carlo sampling to approximate the gradient above which is necessary because we don't know the distribution $p(s|a)$. The use of Monte Carlo follows because we can treat the gradient as an expectation, so by sampling different values of $s$ we can determine the sample mean. In mathematical terms we can do the following:\n",
    "\n",
    "$$ \\frac{\\partial L_s}{\\partial W} \\approx \\frac{1}{N}\\sum_{n=1}^N \\frac{\\partial\\log p(y|\\tilde{s}^n,a)}{\\partial W} +  \\log p(y|\\tilde {s}^n,a)\\frac{\\partial \\log p(\\tilde{s}^n|a)}{\\partial W} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-qualification",
   "metadata": {},
   "source": [
    "The idea is that if we take $N$ large enough then our approximation will be good. It is also important to point out that we sample in the following way:\n",
    "\n",
    "$$ \\tilde{s}^n_t\\sim Multinoulli(\\alpha_t) $$\n",
    "\n",
    "Where $\\alpha_t$ is the output of our attention function inside our network for the given $a$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-wedding",
   "metadata": {},
   "source": [
    "At this point I think the question is how does all of this relate to Reinforcement Learning? In short this is what we have been formulating all along. The method we produced above is (with some modifications) the REINFORCE learning rule, or at least so I am told. In RL we want to learn model weights using a system of reward for a given action. The better the model does at its job the higher reward it will get.\n",
    "\n",
    "To see this we consider the following (which we only mentioned earlier):\n",
    "\n",
    "$$ p(s_t|s_{i<t},a)=\\alpha_t $$\n",
    "\n",
    "The conditional portion of this probability can be seen as the current environment state. The attention information for previous words ($s_{i<t}$) and the input features ($a$) are what we need to make a new decision. The distribution of $\\alpha_t$ is then our policy for making this decision. For example, a policy may be to attend to every feature.\n",
    "\n",
    "So what is the reward for making a given decision? Well, during learning we want the decision to result in the ground-truth output $y$. Thus the reward is...\n",
    "\n",
    "$$ p(y_t|s_{i\\leq t},a) $$\n",
    "\n",
    "This is saying given we chose (according to our policy) to attend in the manner prescribed by $s_t$, what is the probability we produce the correct output? This reward is a value between 0 and 1, with 0 indicating we never produce the right output and 1 indicating we always do. Thus we want to maximize the reward which is essentially maximizing the likelihood of producing the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-tractor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

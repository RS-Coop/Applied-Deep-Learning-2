{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attempted-stockholm",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-pocket",
   "metadata": {},
   "source": [
    "In this notebook we will examine the Cross-Lingual Natural Language Inference ([XNLI](https://cims.nyu.edu/~sbowman/xnli/)) dataset. This is a collection of sentence pairs for textual entailment, so the first sentence either implies, contradicts, or is neutral towards the second sentence. Each pair is then translated from English into 14 other languages ranging from Spanish to Swahili."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-trademark",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-treasurer",
   "metadata": {},
   "source": [
    "To access this dataset we will be using the Hugging Face (HF) [datasets](https://huggingface.co/docs/datasets/index.html) package. The code below will download the data to the *data* folder, and subsequent calls will reuse this data. We note that HF will only allow us to download a single language set per call, so we will start with English as that will be the easiest to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "funny-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "motivated-friendship",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xnli (./data/xnli/en/1.1.0/51ba3a1091acf33fd7c2a54bcbeeee1b1df3ecb127fdca003d31968fa3a1e6a8)\n"
     ]
    }
   ],
   "source": [
    "en = datasets.load_dataset('xnli', 'en', cache_dir='./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-prospect",
   "metadata": {},
   "source": [
    "That went smoothly, so let's examine the data we have obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "after-parliament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "print(en.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grand-contest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Examples: 392702\n",
      "Total Testing Examples: 5010\n",
      "Total Validation Examples: 2490\n"
     ]
    }
   ],
   "source": [
    "print('Total Training Examples:', len(en['train']))\n",
    "print('Total Testing Examples:', len(en['test']))\n",
    "print('Total Validation Examples:', len(en['validation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-charger",
   "metadata": {},
   "source": [
    "We have the three splits one would expect, and we can see that we have almost 400,000 training examples with around 7,500 test/validation examples. We note that each of the examples in the splits above also exists in its 14 translations. As well, the language for the premise and hypothesis can be mixed and matched resulting in a much larger amount of data than might be immediately clear.\n",
    "\n",
    "Below we can see one such example from the validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "proper-rugby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothesis: He called his mom as soon as the school bus dropped him off.\n",
      "label: 1\n",
      "premise: And he said, Mama, I'm home.\n"
     ]
    }
   ],
   "source": [
    "for key, val in en['validation'][0].items():\n",
    "    print(key+':', val) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-rider",
   "metadata": {},
   "source": [
    "Each example has this same structure of hypothesis and premise with a label (in this case 1). Examining the example above, I would expect label 1 to be neutral, but we can confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interior-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> entailment\n",
      "1 -> neutral\n",
      "2 -> contradiction\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(en['validation'].features['label'].names):\n",
    "    print('{} -> {}'.format(i, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-brunswick",
   "metadata": {},
   "source": [
    "Label 0 corresponds to entailment, 1 to neutral (as expected), and 2 to contradiction. For fun lets look at a few more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "identified-shelter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothesis: They told me I should stay home.\n",
      "label: 2\n",
      "premise: They asked a few questions and I answered them and they said, Get your baggage and leave there immediately, and come to the address you were supposed to when you arrived in Washington.\n",
      "\n",
      "\n",
      "hypothesis: It's hard to install the system because hackers attack it every night.\n",
      "label: 1\n",
      "premise: It is not clear the system can be installed before 2010, but even this timetable may be too slow, given the possible security dangers.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(57)\n",
    "samples = random.sample(list(en['validation']), 2)\n",
    "for s in samples:\n",
    "    for key, val in s.items():\n",
    "        print(key+':', val)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-indian",
   "metadata": {},
   "source": [
    "More of what we expect and although this is a small sample size, it appears that the examples are quite easy for a human to understand. I am not sure if there are human performace results for this dataset, but I would expect them to be quite high.\n",
    "\n",
    "The last thing I want to look at in terms of our data is the distibution of classes in the various splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "played-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dist = np.zeros((3,3))\n",
    "for i, key in enumerate(en.keys()):\n",
    "    for item in en[key]:\n",
    "        j = item['label']\n",
    "        dist[i,j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "basic-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entailment</th>\n",
       "      <th>neutral</th>\n",
       "      <th>contradiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>130899.0</td>\n",
       "      <td>130900.0</td>\n",
       "      <td>130903.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>1670.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1670.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>830.0</td>\n",
       "      <td>830.0</td>\n",
       "      <td>830.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            entailment   neutral  contradiction\n",
       "train         130899.0  130900.0       130903.0\n",
       "test            1670.0    1670.0         1670.0\n",
       "validation       830.0     830.0          830.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dist, ['train', 'test', 'validation'], \n",
    "                  en['validation'].features['label'].names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-cheese",
   "metadata": {},
   "source": [
    "That is about as even a distribution as possible, and really we shouldn't expect anything less from such a widely used and well curated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-bride",
   "metadata": {},
   "source": [
    "# Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-vanilla",
   "metadata": {},
   "source": [
    "I am interested in using this dataset by way of a pre-trained model. Specifically, we will consider a [RoBERTa](https://arxiv.org/abs/1911.02116) type model fine-tuned on the XNLI dataset which can be found [here](https://huggingface.co/vicgalle/xlm-roberta-large-xnli-anli).\n",
    "\n",
    "RoBERTa, as its name suggests, is based on the well-known BERT language model. The architecture stays much the same, but a variety of imporved techniques are implemented for training. This includes using significantly more data, refining hyperparameter choices, and changing the training scheme itself. These changes allow much more power to be squeezed out of the same original base. The XLM-RoBERTa model is a sub-category specifically for cross language tasks, and it was trained on data in 100 different languages.\n",
    "\n",
    "The instantiation we will be using is intended for zero-shot text classification which isn't quite our task, but we will adress this later. Some of its specific detials are vauge, but it appears to have been fine-tuned on at least XNLI and ANLI, the latter being an adversarial NLI dataset. Importantly, based on the information available it was not trained on any of the XNLI test data, which (suprisingly) was a problem with some of the other potential options on HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "armed-cowboy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at vicgalle/xlm-roberta-large-xnli-anli and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('zero-shot-classification',\n",
    "                        model='vicgalle/xlm-roberta-large-xnli-anli', device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-cisco",
   "metadata": {},
   "source": [
    "We have loaded our model above which results in the warning about un-trained parameters. As we will see it doesn't seem to make our task impossible, so I will go ahead an ignore that warning. As I mentioned earlier this model is meant to be used for zero-shot text classification: you give it an input, a selection of labels, and then it picks the best one. However, the way it does this is by using the input as a premise and then using a template hypothesis of 'This example is {label}', which is then filled in by the labels. We can use this to implement a work-around by considering the labels to be 'contradicts', 'implies', and 'is neutral towards'. Then we use the template 'This example is {} ' + h, where h is a given hypotheis. An example of this technique can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expected-booth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(I am at home.) implies the hypothesis (Home is where I am.) with probability 0.8172876834869385\n",
      "\n",
      "\n",
      "(I am at home.) contradicts the hypothesis (I am at work.) with probability 0.947782039642334\n",
      "\n",
      "\n",
      "(I am at home.) is neutral towards the hypothesis (I am 22 years old.) with probability 0.36400923132896423\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "premise = 'I am at home.'\n",
    "labels = ['implies', 'is neutral towards', 'contradicts']\n",
    "\n",
    "hypothesis = ['Home is where I am.', 'I am at work.', 'I am 22 years old.']\n",
    "\n",
    "for h in hypothesis:\n",
    "    hyp = 'This example {} ' + h\n",
    "    out = classifier(premise, labels, hyp)\n",
    "    \n",
    "    print(f'({premise}) {out[\"labels\"][0]} the hypothesis ({h}) with probability {out[\"scores\"][0]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-integral",
   "metadata": {},
   "source": [
    "Our premise is 'I am at home.' and then we consider the following three hypothesis: 'Home is where I am.', 'I am not at home.', 'I am 22 years old.'. The first is an implication, the second a contradiction, and the third is neutral. I will note here that the neutral label is not ideal, but there does not seem to be a good English verb for this purpose.\n",
    "\n",
    "We can see that the model gives the correct predicitions in all three cases, although the neutrality case has quite low probability. It seems likely to me that this latter result comes from the poorer quality of the label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-salmon",
   "metadata": {},
   "source": [
    "We will load the XNLI metric to evaluate the performance of our model. In fact, we will use three different metric objects to look independently at the performance on all three classes. Really, these metric objects are just convenience methods for computing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "inner-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load metrics\n",
    "en_metrics = [datasets.load_metric('xnli') for i in range(3)]\n",
    "\n",
    "sw_metrics = [datasets.load_metric('xnli') for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-publicity",
   "metadata": {},
   "source": [
    "I will not consider all 15 languages, but I am interested in looking at the performance as a function of the language resource. We will use English as our high-resource language and Swahili as a low-resource language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "marine-appraisal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xnli (./data/xnli/sw/1.1.0/51ba3a1091acf33fd7c2a54bcbeeee1b1df3ecb127fdca003d31968fa3a1e6a8)\n"
     ]
    }
   ],
   "source": [
    "#Load other languages\n",
    "sw = datasets.load_dataset('xnli', 'sw', cache_dir='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "solved-consensus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hypothesis': 'Sijaongea na yeye tena.', 'label': 2, 'premise': 'Naam, sikukuwa nafikiri juu ya hilo, lakini nilichanganyikiwa sana, na, hatimaye nikaendelea kuzungumza naye tena.'}\n"
     ]
    }
   ],
   "source": [
    "print(sw['test'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-orange",
   "metadata": {},
   "source": [
    "With the Swahili portion loaded we are ready to determine the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gross-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'implies' : 0,\n",
    "    'is neutral towards' : 1,\n",
    "    'contradicts' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "adjusted-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, metrics):\n",
    "    for example in dataset:\n",
    "        premise = example['premise']\n",
    "        h = 'This example {} ' + example['hypothesis']\n",
    "        ref = example['label']\n",
    "\n",
    "        out = classifier(premise, labels, h)\n",
    "\n",
    "        pred = label_map[out['labels'][0]]\n",
    "\n",
    "        metrics[ref].add(prediction=pred, reference=ref)\n",
    "\n",
    "    score = [metric.compute() for metric in metrics]\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "published-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_score = evaluate(en['test'], en_metrics)\n",
    "sw_score = evaluate(sw['test'], sw_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "departmental-success",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: implies\n",
      "English Accuracy: {'accuracy': 0.8772455089820359}\n",
      "Swahili Accuracy: {'accuracy': 0.7604790419161677}\n",
      "\n",
      "\n",
      "Label: is neutral towards\n",
      "English Accuracy: {'accuracy': 0.22994011976047904}\n",
      "Swahili Accuracy: {'accuracy': 0.26706586826347306}\n",
      "\n",
      "\n",
      "Label: contradicts\n",
      "English Accuracy: {'accuracy': 0.45209580838323354}\n",
      "Swahili Accuracy: {'accuracy': 0.5173652694610779}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f'Label: {labels[i]}')\n",
    "    print(f'English Accuracy: {en_score[i]}')\n",
    "    print(f'Swahili Accuracy: {sw_score[i]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-memorabilia",
   "metadata": {},
   "source": [
    "In the results above we can see a number of interesting things. First, the implication accuracy is much better than etheir the neutrality or contradiction. I am not suprised that neutrality has the lowest accuracy, but I would have expected the contradiction accuracy to be higher. Second, accuracy on English is not necessarily higher. It is in the case of implication, but Swahili is better in contradiciton and neutrality.\n",
    "\n",
    "There are a few potential explanations for the results we are seeing. The most obvious is that we aren't really using the model the way it is meant to be used, so that could have some adverse effects. This is especially likely in the case of neutrality where the label is a little funky. I will also say that we are using a pre-trained model from someone on the internet, and with things like the warning we encountered earlier it is hard to say exactly what we are working with. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
